%-----------------------
%\section{Introduction}
%-----------------------
Formal methods provide a mathematically rigorous means of verifying correctness in high-assurance systems, such as those used in the aerospace and defense industries. Certification guidance such as DO-333~\cite{DO-333} explicitly outlines how formal methods can meet airworthiness objectives for commercial aircraft software. Despite their proven effectiveness, adoption within traditional development workflows remains limited, hampered by scalability challenges, poorly designed tooling, and significant barriers to entry due to specialized training requirements~\cite{davis-fmics13}.

The DARPA Pipelined Reasoning of Verifiers Enabling Robust Systems (PRO-VERS) program was launched to address these adoption barriers by developing scalable, human-centered formal verification workflows that seamlessly integrate into existing aerospace and defense engineering practices. Central to PROVERS' objectives is enabling usability even among engineers who lack extensive formal methods expertise, thereby fostering broader adoption and enhancing system dependability.

In response, our team has developed the Industrial-Scale Proof Engineering for Critical Trustworthy Applications (INSPECTA) framework~\cite{inspecta}. INSPECTA comprises two integrated layers—\textit{ProofOps} and \textit{DevOps}—that embed formal verification directly into modern DevOps pipelines. The framework emphasizes scalability and explainability as primary design objectives, aligning closely with the PROVERS program’s goals.

Within INSPECTA’s \textit{ProofOps} workflow, we employ the Assume-Guarantee Reasoning Environment (AGREE)\cite{compositional-analysis-agree}, a compositional verification tool designed specifically for the Architecture Analysis and Design Language (AADL)~\cite{feiler-aadl}.
%
Although AGREE avoids many of the scalability pitfalls found in monolithic verification tools, its counterexample outputs remain difficult to interpret. Like many model checkers, AGREE produces tabular counterexamples that trace the state of variables across multiple time steps. These can involve intricate temporal logic, nested states, and violations spanning architectural layers, posing challenges even for experienced engineers~\cite{cex-explanation}. The diagnostic and repair process may span, hours, days, or weeks for large, evolving models based on user experties.

Recently, generative AI, and particularly large language models (LLMs), have shown promising potential to improve explainability and guide automated formal verification and counterexample repair. Early efforts include OpenAI’s GPT-f, which achieved notable success in Metamath theorem proving~\cite{polu2020generative, megill2019metamath}. Other initiatives have applied LLMs successfully to proof repair in Isabelle/HOL~\cite{first2023baldur}, theorem diagnosis in Coq~\cite{zhang2023getting}, and discovering program invariants~\cite{pei2023can, wu2023lemur}. Stanford and VMware’s Clover project represents another significant step forward, focusing on verifiable code generation with generative assistance~\cite{sun2024clover}. Tahat et al. demonstrated high success rates using multi-turn conversational LLMs for proof repair in Coq, underscoring conversational learning’s value in formal reasoning domains~\cite{CoqDog, CoqDogHCSS24}.
%
Apple's GSM-Symbolic~\cite{mirzadeh2025gsmsymbolic} highlighted fundamental limitations of LLMs in symbolic reasoning tasks. Similarly, Amazon’s recent SMT-backed hallucination prevention framework~\cite{amazon2024mathematical}, while innovative, remains closed-source, available exclusively as a web service, and has yet to integrate within aerospace-specific MBSE pipelines such as those based on AADL.

In this paper, we introduce AGREE-Dog, an open-source generative AI copilot integrated directly into the OSATE IDE. AGREE-Dog is purpose-built to support explainable, compositional reasoning within MBSE workflows, particularly targeting aerospace and defense applications using AADL and AGREE. It automates 16 DevOps and ProofOps steps, including requirements ingestion, context-aware prompt construction, semantic diffing, formal validation, and log analysis. Notably, AGREE-Dog employs a specialized conversational memory management and context-selection approach that efficiently represents evolving verification artifacts in a concise, fixed-size token window, dramatically simplifying the complexity inherent in tracking and repairing system-level contract violations (see Section~\ref{sec:key-impact} %and Appendix~\ref{appendix:test-scenarios} 
for detailed evaluation and artifact descriptions).

To quantify these benefits, we introduce novel structural and temporal evaluation metrics, explicitly addressing aspects such as repair convergence, human intervention level, token efficiency, and response latency.

The rest of this paper is organized as follows. 
Section~\ref{sec:design-architecture} presents AGREE-Dog’s architecture and orchestration strategy. Section~\ref{sec:key-impact} details our experimental evaluation and findings. We conclude with current limitations and outline future work aimed at enhancing autonomy and generalization across verification domains.


