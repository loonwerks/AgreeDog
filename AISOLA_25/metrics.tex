%\section{Evaluation Metrics}
\label{sec:metrics}

This section introduces the core metrics used to evaluate AGREE-Dog’s performance. We organize them into two complementary categories: structural (or spatial) metrics, which quantify the shape and volume of interaction, and temporal metrics, which capture responsiveness and turnaround time. Together, these metrics enable a holistic assessment of automation, effort, and cost.

\subsection{Structural Metrics}

Structural metrics quantify how the repair process unfolds—how many interactions occurred, how much human input was required, and how much computational effort was expended.

\paragraph{\textbf{Total Token Count (TTC)}.}
This metric captures the total number of tokens exchanged during a repair conversation, including both human-authored tokens and those generated by AGREE-Dog—either by the LLM or by the system’s prompt constructor:

\begin{equation}
\text{TTC} = \text{Human Tokens} + \text{AGREE-Dog System Tokens}
\label{eq:ttc}
\end{equation}

TTC serves as a proxy for computational and financial cost (e.g., token-based billing), independent of who authored the tokens. However, it does not by itself distinguish the extent of human involvement.

\paragraph{\textbf{Human Input Ratio (HpR)}.}
This metric measures the proportion of human-authored tokens relative to the total token count:

\begin{equation}
\text{HpR} = \frac{\text{Human-Authored Tokens}}{\text{Total Tokens in Conversation}}
\label{eq:hpr}
\end{equation}

A lower HpR suggests higher automation, with the system contributing more heavily to the conversation. When considered with TTC, this helps differentiate brief, efficient sessions from those with more human effort or verbosity.

\paragraph{\textbf{Number of Repair Cycles (\(N_{\mathrm{RC}}\))}.}
This metric counts the number of conversational cycles required to reach a valid system state:

\begin{equation}
N_{\mathrm{RC}} = \text{Number of Repair Cycles Until Validity}
\label{eq:nrc}
\end{equation}

Each cycle begins with a \texttt{start\_file\_read} message and ends with a \texttt{validity\_status: valid} confirmation. Together, \(N_{\mathrm{RC}}\), HpR, and TTC form a triplet that reflects the intensity, automation level, and computational cost of the repair process.

\paragraph{\textbf{Repair Success Rate (RSR)}.}
This metric measures how often AGREE-Dog succeeds in exactly \(N_{\mathrm{RC}}\) cycles:

\begin{equation}
\text{RSR}(N_{\mathrm{RC}}) = 
\frac{
\text{Number of Tests Solved in } N_{\mathrm{RC}} \text{ Cycles}
}{
\text{Total Number of Tests}
}
\label{eq:rsr}
\end{equation}

\paragraph{\textbf{Cumulative Repair Success Rate} (\( \text{RSR}_{\text{acc}} \)).}
This cumulative variant captures the percentage of tests solved within a given number of cycles:

\begin{equation}
\text{RSR}_{\text{acc}}(N_{\mathrm{RC}}) = 
\frac{
\text{Number of Tests Solved in } \leq N_{\mathrm{RC}} \text{ Cycles}
}{
\text{Total Number of Tests}
}
\label{eq:rsr-acc}
\end{equation}

These success rate metrics extend the basic structural measures to account for convergence and consistency. They are operationalized in Section~\ref{sec:key-impact}, where we analyze repair outcomes and cycle distributions (see Figure~\ref{fig:repair-histogram}).

\subsection{Temporal Metrics}

While structural metrics describe what happened during the interaction, temporal metrics quantify how long it took—enabling assessments grounded in real-world engineering effort and user experience.

\paragraph{\textbf{Wall-Clock Time (WCT)}} The total elapsed time from the first user input to final validation. WCT serves as a practical proxy for engineering effort and turnaround time. Shorter durations may reflect both efficient execution and the usefulness of AGREE-Dog’s guidance.

WCT also conveys a notion of \emph{\textbf{Repair Speed}}—how many valid tasks are completed per unit of time. For example, in our evaluation (Section~\ref{sec:key-impact}), the mean WCT per valid cycle was 2 minutes and 9 seconds, with a median of 1 minute and 39 seconds.


\paragraph{\textbf{LLM Latency}} The average LLM response time per repair cycle. Lower latency improves interactivity and helps maintain user focus, especially in iterative or multi-step sessions.

Next, we define a dependent metric based on the previous temporal measurements to estimate the human return time.

\paragraph{\textbf{Human Return Time (HRT)}} 
This metric estimates the time required for a human to return to a task and make cognitively informed decisions necessary to reach validity during the interaction. It is calculated as the total wall-clock time minus the time AGREE-Dog spends in CPU execution and large language model (LLM) processing. Formally:

\begin{equation}
\text{HRT} = \text{Wall-Clock Time} - \text{CPU Time} - \text{LLM Response Time}
\label{eq:hrt}
\end{equation}


\subsection{Composite Score: Structural and Temporal Dimensions}

To facilitate comprehensive evaluation, we interpret AGREE-Dog's performance using a composite score that integrates both structural and temporal dimensions:

\begin{equation}
\label{eq:composite-score}
(N_{\mathrm{RC}}, \text{HpR}, \text{TTC}, \text{Wall-Clock Time}, \text{LLM Response Latency}, \text{CPU Time})
\end{equation}

This composite vector captures not only the automation level and conciseness of each repair session but also temporal efficiency. For instance, sessions with identical token counts and automation levels might still differ significantly in usability due to variations in latency or total duration. Additionally, this formulation supports the calculation of derived metrics, such as Human Return Time (HRT) (Eq.~\ref{eq:hrt}) and Repair Success Rate (RSR).

By combining structural and temporal perspectives, the composite score provides nuanced insights into human–system interaction dynamics, balancing token efficiency with practical engineering outcomes.